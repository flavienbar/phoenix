{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeframe Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, round, count, sum, max, window, min, last, first\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Spark Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/07 13:07:50 WARN Utils: Your hostname, skynet resolves to a loopback address: 127.0.1.1; using 192.168.1.28 instead (on interface enxa44cc8c105af)\n",
      "24/04/07 13:07:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/07 13:07:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('1min Database Creation')\\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "        .config(\"spark.driver.memory\",\"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the YAML file and load its contents into a dictionary\n",
    "with open('../../references/config_notebook.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Access the variables in the dictionary\n",
    "my_vars = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database with defined timeframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(partition_df, timeframe):\n",
    "    \"\"\"\n",
    "    Process partition DataFrame to calculate aggregated statistics within a given timeframe.\n",
    "\n",
    "    Args:\n",
    "        partition_df (DataFrame): DataFrame representing the partition to be processed.\n",
    "        timeframe (str): Timeframe interval for windowing operations.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed DataFrame with aggregated statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define window specification\n",
    "    window_spec = Window.partitionBy(\"timestamp_interval\")\n",
    "    \n",
    "    # Apply window functions to calculate aggregated statistics\n",
    "    processed_df = partition_df.orderBy('id')\\\n",
    "                    .withColumn(\"timestamp_interval\", window(\"timestamp\", timeframe).start) \\\n",
    "                    .withColumn(\"open\", first(col(\"price\")).over(window_spec)) \\\n",
    "                    .withColumn(\"close\", last(col(\"price\")).over(window_spec)) \\\n",
    "                    .withColumn(\"high\", max(col(\"price\")).over(window_spec))\\\n",
    "                    .withColumn(\"low\", min(col(\"price\")).over(window_spec))\\\n",
    "                    .withColumn(\"qty_sum\", sum(col(\"qty\")).over(window_spec)) \\\n",
    "                    .withColumn(\"quoteQty_sum\", sum(col(\"quoteQty\")).over(window_spec)) \\\n",
    "                    .withColumn(\"transactions_count\", count(col(\"id\")).over(window_spec)) \\\n",
    "                    .withColumn(\"max_quoteQty_sum\", max(col(\"quoteQty\")).over(window_spec)) \\\n",
    "                    .withColumn(\"percentage_of_biggest_transaction\", round(col(\"max_quoteQty_sum\") / col(\"quoteQty_sum\") * 100, 2)) \\\n",
    "                    .withColumn(f\"price_{timeframe}\", round(col(\"quoteQty_sum\") / col(\"qty_sum\"), 2)) \\\n",
    "                    .select(\n",
    "                            col(\"timestamp_interval\").alias(\"timestamp\"),\n",
    "                            col(\"open\"),\n",
    "                            col(\"close\"),\n",
    "                            col(\"high\"),\n",
    "                            col(\"low\"),\n",
    "                            col(\"qty_sum\").alias(\"volume\"),\n",
    "                            col(f\"price_{timeframe}\").alias(\"price\"),\n",
    "                            col(\"quoteQty_sum\").alias(\"quoteQty\"),\n",
    "                            col(\"transactions_count\"),\n",
    "                            col(\"max_quoteQty_sum\"),\n",
    "                            col(\"percentage_of_biggest_transaction\"),\n",
    "                            col(\"zipname\"),\n",
    "                            ) \\\n",
    "                    .dropDuplicates() \\\n",
    "                    .withColumn(\"zipname\", col(\"zipname\").cast(\"string\")) \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_timeframe(source_folder_path, output_folder_path, timeframe):\n",
    "    \"\"\"\n",
    "    Process data from source location and save the processed data to output location to create a database with the interval defined.\n",
    "\n",
    "    Parameters:\n",
    "        source_location (str): Path to the input data source.\n",
    "        output_location (str): Path to save the processed data.\n",
    "        timeframe (str): Time interval in minutes for grouping the data. need to be valided with the pyspark window function\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize Spark session\n",
    "    \"\"\"spark = SparkSession.builder.appName(\"process_data\")\\\n",
    "                .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "                .config(\"spark.driver.memory\",\"4g\") \\\n",
    "                .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "                .getOrCreate()\"\"\"\n",
    "\n",
    "    # Read data from source location\n",
    "    df = spark.read.parquet(os.path.join(source_folder_path, \"BTCUSDT.parquet\"))\n",
    "    \n",
    "    # Group the dataframe by the partition column\n",
    "    partitions = df.select(\"zipname\").distinct().collect()\n",
    "\n",
    "    for partition in partitions:\n",
    "        partition_df = df.filter(col(\"zipname\") == partition.zipname)\n",
    "        processed_df = process_partition(partition_df, timeframe)\n",
    "        \n",
    "        # Save the data as a partitioned Parquet file based on the zip filename\n",
    "        output_path = os.path.join(output_folder_path, f\"BTCUSDT_{timeframe.replace(' ', '_')}.parquet\")\n",
    "\n",
    "        # Save processed data to output location\n",
    "        processed_df.write \\\n",
    "            .partitionBy(\"zipname\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"compression\", \"gzip\") \\\n",
    "            .option(\"blockSize\", \"256m\") \\\n",
    "            .parquet(output_path)\n",
    "\n",
    "        # Stop Spark session\n",
    "        # spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the database with a defined timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "source_folder_path = my_vars[\"TEST\"]\n",
    "output_folder_path = my_vars[\"TEST\"]\n",
    "timeframe='1 second'\n",
    "database_timeframe(source_folder_path, output_folder_path, timeframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+--------+--------------------+--------+------------------+------------------+----------------+---------------------------------+--------+\n",
      "|          timestamp|    open|   close|    high|     low|              volume|   price|          quoteQty|transactions_count|max_quoteQty_sum|percentage_of_biggest_transaction| zipname|\n",
      "+-------------------+--------+--------+--------+--------+--------------------+--------+------------------+------------------+----------------+---------------------------------+--------+\n",
      "|2023-03-29 00:00:05|27262.38|27262.38|27262.38|27262.37| 0.04384000000000001|27262.38|1195.1826605999997|                17|     183.7484412|                            15.37|20230329|\n",
      "|2023-03-29 00:00:06|27262.37|27262.38|27262.38|27262.37| 0.41868999999999995|27262.37|     11414.4832293|                21|     2472.696959|                            21.66|20230329|\n",
      "|2023-03-29 00:00:08|27266.64|27266.63|27266.64|27266.63| 0.11696000000000001|27266.64|        3189.10574|                12|    1132.6558102|                            35.52|20230329|\n",
      "|2023-03-29 00:00:10|27266.63|27266.63|27266.64|27266.63|             0.05092|27266.63|      1388.4169545|                 6|     475.5300272|                            34.25|20230329|\n",
      "|2023-03-29 00:00:20|27273.48| 27275.5| 27275.5|27273.47| 0.08102999999999999|27274.07|      2210.0182442|                35|       887.76219|                            40.17|20230329|\n",
      "|2023-03-29 00:00:25|27281.58|27282.13|27282.14|27281.58|             0.40467| 27282.1|      11040.247372|                34|    4131.6057672|                            37.42|20230329|\n",
      "|2023-03-29 00:00:29|27284.75|27284.76|27284.76|27284.75|  0.5573400000000001|27284.75|     15206.8827546|                 5|     14397.61688|                            94.68|20230329|\n",
      "|2023-03-29 00:00:34|27284.76|27284.75|27284.76|27284.75|0.040979999999999996|27284.76|      1118.1293303|                 6|     561.5203608|                            50.22|20230329|\n",
      "|2023-03-29 00:00:44|27284.75|27284.75|27284.75|27284.75|              8.9E-4|27284.75|        24.2834275|                 1|      24.2834275|                            100.0|20230329|\n",
      "|2023-03-29 00:01:00|27265.72|27265.73|27265.73|27265.72|0.028650000000000002|27265.73| 781.1630234999999|                14|     125.6950153|                            16.09|20230329|\n",
      "|2023-03-29 00:01:15|27261.07|27261.08|27261.08|27261.07|             0.01447|27261.07|       394.4677109|                 6|     148.8454422|                            37.73|20230329|\n",
      "|2023-03-29 00:01:28|27256.77|27256.76|27256.77|27256.76|             0.01247|27256.76|       339.8918377|                11|       95.398695|                            28.07|20230329|\n",
      "|2023-03-29 00:01:57|27264.62|27264.61|27264.62|27264.61|              0.0319|27264.62|       869.7413714|                 2|     851.7467288|                            97.93|20230329|\n",
      "|2023-03-29 00:01:59|27264.62|27264.61|27264.62|27264.61|             0.03838|27264.61|1046.4158613999998|                19|     149.1374167|                            14.25|20230329|\n",
      "|2023-03-29 00:02:40|27269.78|27269.77|27269.78|27269.77|             0.09153|27269.78|      2496.0029531|                 4|    2017.6910222|                            80.84|20230329|\n",
      "|2023-03-29 00:02:46|27275.55|27275.54|27275.55|27275.54|             0.00926|27275.54|       252.5715152|                 6|      79.9173322|                            31.64|20230329|\n",
      "|2023-03-29 00:03:03|27275.54|27275.55|27275.55|27275.54| 0.13752999999999999|27275.55|      3751.2062562|                 3|       3382.1682|                            90.16|20230329|\n",
      "|2023-03-29 00:03:07|27275.55|27275.55|27275.55|27275.54|0.016909999999999998|27275.54|461.22941209999993|                12|     131.7408582|                            28.56|20230329|\n",
      "|2023-03-29 00:03:12|27275.55|27275.55|27275.55|27275.55|             0.00971|27275.55|       264.8455905|                 4|       99.828513|                            37.69|20230329|\n",
      "|2023-03-29 00:03:22|27279.42|27279.42|27279.42|27279.41|             0.00791|27279.42|       215.7801988|                 4|     100.3882656|                            46.52|20230329|\n",
      "+-------------------+--------+--------+--------+--------+--------------------+--------+------------------+------------------+----------------+---------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(f'{output_folder_path}/BTCUSDT_1_second.parquet')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quoteQty: double (nullable = true)\n",
      " |-- transactions_count: long (nullable = true)\n",
      " |-- max_quoteQty_sum: double (nullable = true)\n",
      " |-- percentage_of_biggest_transaction: double (nullable = true)\n",
      " |-- zipname: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+---------------------------------+-----------+\n",
      "|summary|              open|             close|              high|               low|            volume|             price|          quoteQty|transactions_count|  max_quoteQty_sum|percentage_of_biggest_transaction|    zipname|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+---------------------------------+-----------+\n",
      "|  count|             83171|             83171|             83171|             83171|             83171|             83171|             83171|             83171|             83171|                            83171|      83171|\n",
      "|   mean|28098.613981916747|28098.626424354577| 28099.00647713739|28098.232973392136|1.0759298322732682|28098.619767466982| 30297.67426400023| 20.49051953204843| 7150.927420436078|                52.39465041901629|2.0230329E7|\n",
      "| stddev|  419.588750563461| 419.5800699898428|419.63624109904805|419.53145400684645| 3.664103709609626|419.58096680902264|103041.55513739171| 46.47941869867183|15467.342082660462|               27.529373861043155|        0.0|\n",
      "|    min|          27240.11|           27240.1|          27240.11|           27240.1|            2.2E-4|          27240.11|         6.0036878|                 1|         6.0036878|                             1.62|   20230329|\n",
      "|    max|           28641.3|           28641.3|           28650.0|          28633.56|193.19152999999997|          28644.94| 5372695.079886609|              3320|   1199933.8601294|                            100.0|   20230329|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+---------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thefair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
