{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 minute Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, round, count, sum, max, window\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Spark Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/06 12:56:31 WARN Utils: Your hostname, skynet resolves to a loopback address: 127.0.1.1; using 192.168.1.12 instead (on interface wlp2s0)\n",
      "24/04/06 12:56:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/06 12:56:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('1min Database Creation')\\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "        .config(\"spark.driver.memory\",\"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the YAML file and load its contents into a dictionary\n",
    "with open('../../references/config_notebook.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Access the variables in the dictionary\n",
    "my_vars = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database with defined timeframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(partition_df, timeframe):\n",
    "    window_spec = Window.partitionBy(\"timestamp_interval\")\n",
    "\n",
    "    # Read data from source location and process\n",
    "    processed_df = partition_df.withColumn(\"timestamp_interval\", window(\"timestamp\", timeframe).start) \\\n",
    "            .withColumn(\"qty_sum\", sum(col(\"qty\")).over(window_spec)) \\\n",
    "            .withColumn(\"quoteQty_sum\", sum(col(\"quoteQty\")).over(window_spec)) \\\n",
    "            .withColumn(\"transactions_count\", count(col(\"id\")).over(window_spec)) \\\n",
    "            .withColumn(\"max_quoteQty_sum\", max(col(\"quoteQty\")).over(window_spec)) \\\n",
    "            .withColumn(\"percentage_of_biggest_transaction\", round(col(\"max_quoteQty_sum\") / col(\"quoteQty_sum\") * 100, 2)) \\\n",
    "            .withColumn(f\"price_{timeframe}\", round(col(\"quoteQty_sum\") / col(\"qty_sum\"), 2)) \\\n",
    "            .select(\n",
    "                col(f\"price_{timeframe}\").alias(\"price\"),\n",
    "                col(\"qty_sum\").alias(\"qty\"),\n",
    "                col(\"quoteQty_sum\").alias(\"quoteQty\"),\n",
    "                col(\"timestamp_interval\").alias(\"timestamp\"),\n",
    "                col(\"transactions_count\"),\n",
    "                col(\"max_quoteQty_sum\"),\n",
    "                col(\"percentage_of_biggest_transaction\")\n",
    "            ).dropDuplicates([\"price\", \"qty\", \"quoteQty\", \"timestamp\", \"transactions_count\", \"max_quoteQty_sum\", \"percentage_of_biggest_transaction\"]) \\\n",
    "            .withColumn(\"zipname\", date_format(col(\"timestamp\"), \"yyyyMMdd\")) \\\n",
    "            .orderBy(\"timestamp\")\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "\n",
    "\n",
    "def database_timeframe(source_folder_path, output_folder_path, timeframe):\n",
    "    \"\"\"\n",
    "    Process data from source location and save the processed data to output location to create a database with the interval defined.\n",
    "\n",
    "    Parameters:\n",
    "        source_location (str): Path to the input data source.\n",
    "        output_location (str): Path to save the processed data.\n",
    "        timeframe (str): Time interval in minutes for grouping the data. need to be valided with the pyspark window function\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize Spark session\n",
    "    \"\"\"spark = SparkSession.builder.appName(\"process_data\")\\\n",
    "                .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "                .config(\"spark.driver.memory\",\"4g\") \\\n",
    "                .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "                .getOrCreate()\"\"\"\n",
    "\n",
    "    # Read data from source location\n",
    "    df = spark.read.parquet(os.path.join(source_folder_path, \"BTCUSDT.parquet\"))\n",
    "    \n",
    "    # Group the dataframe by the partition column\n",
    "    partitions = df.select(\"zipname\").distinct().collect()\n",
    "\n",
    "    for partition in partitions:\n",
    "        partition_df = df.filter(col(\"zipname\") == partition.zipname)\n",
    "        processed_df = process_partition(partition_df, timeframe)\n",
    "        \n",
    "        # Save the data as a partitioned Parquet file based on the zip filename\n",
    "        output_path = os.path.join(output_folder_path, f\"BTCUSDT_{timeframe.replace(' ', '_')}.parquet\")\n",
    "        \n",
    "\n",
    "        # Save processed data to output location\n",
    "        processed_df.write \\\n",
    "            .partitionBy(\"zipname\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"compression\", \"gzip\") \\\n",
    "            .option(\"blockSize\", \"256m\") \\\n",
    "            .parquet(output_path)\n",
    "\n",
    "        # Stop Spark session\n",
    "        # spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the database with a defined timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "source_folder_path = my_vars[\"DATA\"][\"external\"]\n",
    "output_folder_path = my_vars[\"DATA\"][\"external\"]\n",
    "timeframe='1 second'\n",
    "database_timeframe(source_folder_path, output_folder_path, timeframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+-------------------+------------------+----------------+---------------------------------+--------+\n",
      "|   price|               qty|          quoteQty|          timestamp|transactions_count|max_quoteQty_sum|percentage_of_biggest_transaction| zipname|\n",
      "+--------+------------------+------------------+-------------------+------------------+----------------+---------------------------------+--------+\n",
      "|33354.74| 74.62069999999994|2488954.2309403503|2021-06-09 00:00:00|              1588|  54981.37727429|                             2.21|20210609|\n",
      "|33490.19| 90.64387200000004| 3035680.930924956|2021-06-09 00:01:00|              1717|  74799.75750877|                             2.46|20210609|\n",
      "|33467.67|55.995943999999994|1874053.9825789297|2021-06-09 00:02:00|              1003|     106405.4811|                             5.68|20210609|\n",
      "|33481.16| 39.98078699999992|1338602.9591662413|2021-06-09 00:03:00|               963|   54876.3919872|                              4.1|20210609|\n",
      "|33437.94|26.282152999999997|  878821.057304849|2021-06-09 00:04:00|               953|  31331.23914595|                             3.57|20210609|\n",
      "|33247.12|        201.246681| 6690871.729756731|2021-06-09 00:05:00|              3537|    99999.993166|                             1.49|20210609|\n",
      "|33162.59|105.46386399999986|  3497454.68685473|2021-06-09 00:06:00|              2347|    142255.63365|                             4.07|20210609|\n",
      "|33119.13|115.48419200000006| 3824735.475099416|2021-06-09 00:07:00|              2009| 259483.57216695|                             6.78|20210609|\n",
      "|33081.21| 125.0376130000001|  4136395.85412276|2021-06-09 00:08:00|              2198|      244747.809|                             5.92|20210609|\n",
      "|33099.05| 73.82875100000011| 2443661.534740046|2021-06-09 00:09:00|              1594| 116903.47181766|                             4.78|20210609|\n",
      "| 33152.5|32.424407999999985|1074950.3261285897|2021-06-09 00:10:00|              1021|  35517.44224672|                              3.3|20210609|\n",
      "|33187.48|29.190675000000045| 968764.8458684799|2021-06-09 00:11:00|               814|  61501.72152486|                             6.35|20210609|\n",
      "|33152.31| 37.52686000000007|1244102.0954986205|2021-06-09 00:12:00|               960| 118004.42955557|                             9.49|20210609|\n",
      "|33145.23| 39.93329700000002|1323598.2870416301|2021-06-09 00:13:00|               816| 133088.07795924|                            10.06|20210609|\n",
      "|33105.83|52.919572000000045| 1751946.284922193|2021-06-09 00:14:00|               919| 102698.09462851|                             5.86|20210609|\n",
      "|33115.88| 46.50352299999994| 1540005.227093299|2021-06-09 00:15:00|              1155| 153823.68137304|                             9.99|20210609|\n",
      "|33145.94|         58.881511|1951682.7549828999|2021-06-09 00:16:00|              1001|  85022.57911989|                             4.36|20210609|\n",
      "|33140.16|41.890764000000075|  1388266.58709841|2021-06-09 00:17:00|               810|  45818.72277836|                              3.3|20210609|\n",
      "|33079.68| 48.98653400000006|1620458.6693751407|2021-06-09 00:18:00|              1078|  50988.76565925|                             3.15|20210609|\n",
      "|32982.76|215.56911799999992|7110064.1191795785|2021-06-09 00:19:00|              3980|   538695.128796|                             7.58|20210609|\n",
      "+--------+------------------+------------------+-------------------+------------------+----------------+---------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(f'{output_folder_path}/BTCUSDT_1_minute.parquet')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- price: double (nullable = true)\n",
      " |-- qty: double (nullable = true)\n",
      " |-- quoteQty: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- transactions_count: long (nullable = true)\n",
      " |-- max_quoteQty_sum: double (nullable = true)\n",
      " |-- percentage_of_biggest_transaction: double (nullable = true)\n",
      " |-- zipname: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9024:=================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+------------------+------------------+---------------------------------+------------------+\n",
      "|summary|             price|               qty|            quoteQty|transactions_count|  max_quoteQty_sum|percentage_of_biggest_transaction|           zipname|\n",
      "+-------+------------------+------------------+--------------------+------------------+------------------+---------------------------------+------------------+\n",
      "|  count|           1621814|           1621814|             1621814|           1621814|           1621814|                          1621814|           1621814|\n",
      "|   mean| 36020.10351938636| 70.25208871442777|  2020457.4650399673|1755.9275107996355| 76785.02199562339|                5.948698068952428|2.02228028347776E7|\n",
      "| stddev|13590.425674918739|120.81812867913322|  2985708.5204539527|2308.3714501456234|130965.68276975716|               5.0777179637939245| 9241.306183481804|\n",
      "|    min|          15511.42|            0.2597|   7084.981544500002|                58|       599.1527875|                             0.09|          20210301|\n",
      "|    max|          73721.79| 5877.775450000211|1.4595566833284837E8|            107315|   3.38029281194E7|                            92.94|          20240331|\n",
      "+-------+------------------+------------------+--------------------+------------------+------------------+---------------------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thefair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
