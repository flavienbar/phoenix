{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeframe Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, round, count, sum, max, window, min, last, first\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Spark Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('1min Database Creation')\\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "        .config(\"spark.driver.memory\",\"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the YAML file and load its contents into a dictionary\n",
    "with open('../../references/config_notebook.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Access the variables in the dictionary\n",
    "my_vars = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database with defined timeframe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(partition_df, timeframe):\n",
    "    \"\"\"\n",
    "    Process partition DataFrame to calculate aggregated statistics within a given timeframe.\n",
    "\n",
    "    Args:\n",
    "        partition_df (DataFrame): DataFrame representing the partition to be processed.\n",
    "        timeframe (str): Timeframe interval for windowing operations.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed DataFrame with aggregated statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define window specification\n",
    "    window_spec = Window.partitionBy(\"timestamp_interval\")\n",
    "    \n",
    "    # Apply window functions to calculate aggregated statistics\n",
    "    processed_df = partition_df.orderBy('id')\\\n",
    "                    .withColumn(\"timestamp_interval\", window(\"timestamp\", timeframe).start) \\\n",
    "                    .withColumn(\"open\", first(col(\"price\")).over(window_spec)) \\\n",
    "                    .withColumn(\"close\", last(col(\"price\")).over(window_spec)) \\\n",
    "                    .withColumn(\"high\", max(col(\"price\")).over(window_spec))\\\n",
    "                    .withColumn(\"low\", min(col(\"price\")).over(window_spec))\\\n",
    "                    .withColumn(\"qty_sum\", sum(col(\"qty\")).over(window_spec)) \\\n",
    "                    .withColumn(\"quoteQty_sum\", sum(col(\"quoteQty\")).over(window_spec)) \\\n",
    "                    .withColumn(\"transactions_count\", count(col(\"id\")).over(window_spec)) \\\n",
    "                    .withColumn(\"max_quoteQty_sum\", max(col(\"quoteQty\")).over(window_spec)) \\\n",
    "                    .withColumn(\"percentage_of_biggest_transaction\", round(col(\"max_quoteQty_sum\") / col(\"quoteQty_sum\") * 100, 2)) \\\n",
    "                    .withColumn(f\"price_{timeframe}\", round(col(\"quoteQty_sum\") / col(\"qty_sum\"), 2)) \\\n",
    "                    .select(\n",
    "                            col(\"timestamp_interval\").alias(\"timestamp\"),\n",
    "                            col(\"open\"),\n",
    "                            col(\"close\"),\n",
    "                            col(\"high\"),\n",
    "                            col(\"low\"),\n",
    "                            col(\"qty_sum\").alias(\"volume\"),\n",
    "                            col(f\"price_{timeframe}\").alias(\"price\"),\n",
    "                            col(\"quoteQty_sum\").alias(\"quoteQty\"),\n",
    "                            col(\"transactions_count\"),\n",
    "                            col(\"max_quoteQty_sum\"),\n",
    "                            col(\"percentage_of_biggest_transaction\"),\n",
    "                            col(\"zipname\"),\n",
    "                            ) \\\n",
    "                    .dropDuplicates() \\\n",
    "                    .withColumn(\"zipname\", col(\"zipname\").cast(\"string\")) \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_timeframe(source_folder_path, output_folder_path, timeframe):\n",
    "    \"\"\"\n",
    "    Process data from source location and save the processed data to output location to create a database with the interval defined.\n",
    "\n",
    "    Parameters:\n",
    "        source_location (str): Path to the input data source.\n",
    "        output_location (str): Path to save the processed data.\n",
    "        timeframe (str): Time interval in minutes for grouping the data. need to be valided with the pyspark window function\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize Spark session\n",
    "    \"\"\"spark = SparkSession.builder.appName(\"process_data\")\\\n",
    "                .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "                .config(\"spark.driver.memory\",\"4g\") \\\n",
    "                .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "                .getOrCreate()\"\"\"\n",
    "\n",
    "    # Read data from source location\n",
    "    df = spark.read.parquet(os.path.join(source_folder_path, \"BTCUSDT.parquet\"))\n",
    "    \n",
    "    # Group the dataframe by the partition column\n",
    "    partitions = df.select(\"zipname\").distinct().collect()\n",
    "\n",
    "    for partition in partitions:\n",
    "        partition_df = df.filter(col(\"zipname\") == partition.zipname)\n",
    "        processed_df = process_partition(partition_df, timeframe)\n",
    "        \n",
    "        # Save the data as a partitioned Parquet file based on the zip filename\n",
    "        output_path = os.path.join(output_folder_path, f\"BTCUSDT_{timeframe.replace(' ', '_')}.parquet\")\n",
    "\n",
    "        # Save processed data to output location\n",
    "        processed_df.repartition(1).write \\\n",
    "            .partitionBy(\"zipname\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"compression\", \"gzip\") \\\n",
    "            .option(\"blockSize\", \"256m\") \\\n",
    "            .parquet(output_path)\n",
    "\n",
    "        # Stop Spark session\n",
    "        # spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the database with a defined timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "source_folder_path = my_vars['DATA']['external']\n",
    "output_folder_path = my_vars['DATA']['processed']\n",
    "timeframe='1 second'\n",
    "database_timeframe(source_folder_path, output_folder_path, timeframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+--------+------------------+--------+--------------------+------------------+----------------+---------------------------------+--------+\n",
      "|          timestamp|    open|   close|    high|     low|            volume|   price|            quoteQty|transactions_count|max_quoteQty_sum|percentage_of_biggest_transaction| zipname|\n",
      "+-------------------+--------+--------+--------+--------+------------------+--------+--------------------+------------------+----------------+---------------------------------+--------+\n",
      "|2021-05-21 00:01:00|40673.45|40853.92|40867.65| 40578.5| 171.4949500000004|40723.86|   6983936.847300684|              4219|  87508.60495188|                             1.25|20210521|\n",
      "|2021-05-21 00:14:00|40935.82|41024.99|41039.39|40920.32| 84.65311099999994|40973.98|  3468574.6886659954|              2046|  79115.03998311|                             2.28|20210521|\n",
      "|2021-05-21 00:24:00| 40904.8|40939.11| 41058.7|40891.41| 82.68007500000016|40987.49|    3388849.08148309|              2689|       99999.984|                             2.95|20210521|\n",
      "|2021-05-21 00:31:00|41331.66|41299.14|41393.66|41218.97|177.93620799999945| 41313.5|   7351166.660687785|              4596|    160810.07166|                             2.19|20210521|\n",
      "|2021-05-21 00:37:00|41255.63| 41250.0|41271.46| 41182.6| 50.55229999999998| 41223.5|   2083942.689963897|              1654|     51469.52928|                             2.47|20210521|\n",
      "|2021-05-21 00:42:00|41877.82|41942.31| 42000.0|41826.91|450.11951899999946|41969.45|1.8891268749768388E7|              8687|  249299.4638685|                             1.32|20210521|\n",
      "|2021-05-21 00:50:00|41709.59|41650.01| 41711.2|41629.26| 69.81269299999998|41673.55|   2909342.730647654|              2161|  99999.97400016|                             3.44|20210521|\n",
      "|2021-05-21 00:51:00|41650.01|41767.81|41808.61| 41600.0|118.34254299999992|41704.64|    4935433.04572995|              2881| 144942.04733939|                             2.94|20210521|\n",
      "|2021-05-21 01:19:00|41601.99|41549.37|41617.69|41525.31| 35.20956299999999|41565.65|  1463508.3013957997|              1819|   77908.2443454|                             5.32|20210521|\n",
      "|2021-05-21 01:21:00|41506.24|41523.51|41572.65|41470.36|51.527289999999965|41526.91|  2139769.2891484494|              1685|        61698.05|                             2.88|20210521|\n",
      "|2021-05-21 01:22:00|41525.59|41563.68|41634.82|41520.74| 48.31229499999999|41588.21|  2009221.8013775772|              1477|      99999.9936|                             4.98|20210521|\n",
      "|2021-05-21 01:33:00|41643.89| 41639.0|41682.68|41610.66| 45.76563100000003|41646.55|  1905980.5903884482|              1448|    84532.793643|                             4.44|20210521|\n",
      "|2021-05-21 01:39:00|41673.45|41729.72|41738.02|41649.88| 66.09657300000003|41701.28|  2756311.4611663083|              1865| 206982.88098646|                             7.51|20210521|\n",
      "|2021-05-21 01:54:00|41435.06|41461.55|41494.13|41404.13| 79.28749300000007|41431.67|  3285013.4232229413|              1883|      256870.836|                             7.82|20210521|\n",
      "|2021-05-21 02:29:00|41189.03|41178.18|41198.67|41091.52|35.360452000000016|41143.99|  1454870.0388018792|              1043|  99999.97821152|                             6.87|20210521|\n",
      "|2021-05-21 02:37:00|41212.46|41279.53|41296.29|41212.46| 36.98885099999998|41269.61|  1526515.5031276243|               924|   99999.9899822|                             6.55|20210521|\n",
      "|2021-05-21 02:40:00|41208.21|41166.08|41257.86|41164.02|40.675616000000026| 41213.9|   1676400.609659391|               941|    108244.04976|                             6.46|20210521|\n",
      "|2021-05-21 02:42:00|41230.44|41236.19|41296.95|41175.32|40.371437000000086|41244.02|  1665080.5522560694|              1152| 101006.97909265|                             6.07|20210521|\n",
      "|2021-05-21 02:59:00|41273.31|41248.33|41275.82|41213.94| 35.24202700000001|41243.77|  1453514.0110020705|               836|  73802.39037434|                             5.08|20210521|\n",
      "|2021-05-21 03:05:00|41033.82|40979.53|41053.48|40890.24|  87.1794979999998|40963.89|   3571211.715208565|              3460|     157085.7015|                              4.4|20210521|\n",
      "+-------------------+--------+--------+--------+--------+------------------+--------+--------------------+------------------+----------------+---------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(f'{output_folder_path}/BTCUSDT_1_minute.parquet')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quoteQty: double (nullable = true)\n",
      " |-- transactions_count: long (nullable = true)\n",
      " |-- max_quoteQty_sum: double (nullable = true)\n",
      " |-- percentage_of_biggest_transaction: double (nullable = true)\n",
      " |-- zipname: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12405:================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+------------------+------------------+---------------------------------+------------------+\n",
      "|summary|              open|             close|             high|               low|            volume|             price|            quoteQty|transactions_count|  max_quoteQty_sum|percentage_of_biggest_transaction|           zipname|\n",
      "+-------+------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+------------------+------------------+---------------------------------+------------------+\n",
      "|  count|           1621814|           1621814|          1621814|           1621814|           1621814|           1621814|             1621814|           1621814|           1621814|                          1621814|           1621814|\n",
      "|   mean|36020.098661770026|  36020.1126074999|36038.45462315667| 36001.77027281142| 70.25208871442783| 36020.10351938634|  2020457.4650399645|1755.9275107996355| 76785.02199562333|                 5.94869806895242|2.02228028347776E7|\n",
      "| stddev|13590.513225512068|13590.539954101598|13599.46249873519|13581.321597961938|120.81812867913324|13590.425674918744|  2985708.5204539574|2308.3714501456207|130965.68276975703|               5.0777179637939245| 9241.306183487566|\n",
      "|    min|          15513.84|          15513.84|         15544.47|           15476.0|            0.2597|          15511.42|   7084.981544500002|                58|       599.1527875|                             0.09|          20210301|\n",
      "|    max|          73775.55|          73775.54|          73777.0|          73682.61| 5877.775450000211|          73721.79|1.4595566833284837E8|            107315|   3.38029281194E7|                            92.94|          20240331|\n",
      "+-------+------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+------------------+------------------+---------------------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thefair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
